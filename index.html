<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.15"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>GStreamer Video Analytics (GVA) Plugin: GStreamer Video Analytics API reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">GStreamer Video Analytics (GVA) Plugin
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.15 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">GStreamer Video Analytics API reference </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Overview</h2>
<p>This documentation describes Gstreamer Video Analytics metadata API (Application Programming Interface) that allows you to access and to control inference results obtained from GVA plugin elements, such as <b>gvainference</b>, <b>gvadetect</b>, <b>gvaclassify</b>. In GVA plugin, all inference results (both raw and interpreted) are passing through GStreamer pipeline being attached to GstBuffer objects as GStreamer metadata.</p>
<p>For better developer experience we provide GVA API (<b>C++</b> and <b>Python</b> versions) which simplifies any job you want to perform on inference results, like consuming and further reusing, additional custom post-processing, and more. Also, most of the times, GVA API allows you to abstract from internal inference results as GStreamer metadata.</p>
<p>In GVA API, most of the job is done by these classes: <a class="el" href="classGVA_1_1VideoFrame.html" title="This class represents video frame - object for working with RegionOfInterest and Tensor objects which...">GVA::VideoFrame</a>, <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a> &amp; <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a> (<b>C++</b> version is referenced here, but <b>Python</b> version is available as well). Take a look at schematic picture below, which represents how these classes relate. </p><div class="image">
<img src="GVA_API.png" alt="GVA_API.png"/>
</div>
   <p>On this picture you can see 3 classes mentioned above. <a class="el" href="classGVA_1_1VideoFrame.html" title="This class represents video frame - object for working with RegionOfInterest and Tensor objects which...">GVA::VideoFrame</a> is the most high-level object, which is constructed from GstBuffer. It means that <a class="el" href="classGVA_1_1VideoFrame.html" title="This class represents video frame - object for working with RegionOfInterest and Tensor objects which...">GVA::VideoFrame</a> represents one image (one video frame). One <a class="el" href="classGVA_1_1VideoFrame.html" title="This class represents video frame - object for working with RegionOfInterest and Tensor objects which...">GVA::VideoFrame</a> can contain from 0 to many <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a> objects (ROI on picture above). <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a> describes detected object, e.g. its bounding box coordinates and label. Number of <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a> objects added to <a class="el" href="classGVA_1_1VideoFrame.html" title="This class represents video frame - object for working with RegionOfInterest and Tensor objects which...">GVA::VideoFrame</a> depends on whether detection already happened on this image (by <b>gvadetect</b> element in a pipeline) and whether this image contains patterns which are recognized by detection models (for example, it could be image with humans faces and detection model, which was trained to detect faces).</p>
<p>Similar, one <a class="el" href="classGVA_1_1VideoFrame.html" title="This class represents video frame - object for working with RegionOfInterest and Tensor objects which...">GVA::VideoFrame</a> can contain from 0 to many <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a> objects, which are products of running inference (by <b>gvainference</b> element in a pipeline) on full video frame. Such <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a> contains raw inference result which can be consumed and post-processed (interpreted) any way you like. For example, if you have custom detection model, which is not supported by <b>gvadetect</b>, you can run inference with <b>gvainference</b> element in a pipeline, and obtain results as a <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a> object. You can then use these results for getting regions of interest of this frame with your post-processing algorithm, and add your own <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a> to <a class="el" href="classGVA_1_1VideoFrame.html" title="This class represents video frame - object for working with RegionOfInterest and Tensor objects which...">GVA::VideoFrame</a> with <a class="el" href="classGVA_1_1VideoFrame.html#a76725cfebba9bc9dce43edd80d9b31ac" title="Attach RegionOfInterest to this VideoFrame. This function takes ownership of region_tensor,...">GVA::VideoFrame::add_region</a>. Below you can find <a href="#tutorial">tutorial</a> devoted to this use case.</p>
<p><a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a> can also contain multiple <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a> objects, which are products of running classification (by <b>gvaclassify</b> element in a pipeline) on <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a> they belong to. Such <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a> can contain <a class="el" href="classGVA_1_1Tensor.html#a81285ccb826dd979f0b77fa8361a7f06" title="Get label. This label is set for Tensor instances produced by gvaclassify element....">GVA::Tensor::label</a> which is string containing interpreted classification result (examples are person's age, car's color, etc.). <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a> also stores raw data (model output blob), which can be obtained with <a class="el" href="classGVA_1_1Tensor.html#ac61de94784df20b83e2b045b26b48506" title="Get raw inference result blob data.">GVA::Tensor::data</a>. Normally, one <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a> contains some additional detection information, which can be obtained with <a class="el" href="classGVA_1_1RegionOfInterest.html#aa3360d1e9ffd9f4921905cfc756d2f4a" title="Returns detection Tensor, last added to this RegionOfInterest. As any other Tensor,...">GVA::RegionOfInterest::detection</a>. You can check if <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a> is detection tensor with <a class="el" href="classGVA_1_1Tensor.html#a28a71b8b0973df2df5707bb74b1588f7" title="Check if this Tensor is detection Tensor (contains detection results)">GVA::Tensor::is_detection</a>. Detection <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a> extends detection information contained in <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a>.</p>
<p>Any modification you perform using GVA API will affect underlying metadata as though you modified it directly. For example, you can add <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a> or <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a> objects to <a class="el" href="classGVA_1_1VideoFrame.html" title="This class represents video frame - object for working with RegionOfInterest and Tensor objects which...">GVA::VideoFrame</a>, and real GStreamer metadata instances will be added to GstBuffer of current <a class="el" href="classGVA_1_1VideoFrame.html" title="This class represents video frame - object for working with RegionOfInterest and Tensor objects which...">GVA::VideoFrame</a>. It means, the objects you added will behave as if they were produced by GVA elements mentioned above. You will be able to reach these objects futher by pipeline using both internal metadata representation and GVA API. Also, any GVA elements that rely on inference results will employ objects you added. For example, <b>gvawatermark</b> will render on screen these objects (for example, it will draw bounding box for added <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a>).</p>
<p>It is important to say, that GVA API, if used, expects that it is used consistently. It means, that you should not operate on metadata with GStreamer C calls and GVA API at the same time. As an example, if you're adding some regions of interest to GstBuffer with <a class="el" href="classGVA_1_1VideoFrame.html#a76725cfebba9bc9dce43edd80d9b31ac" title="Attach RegionOfInterest to this VideoFrame. This function takes ownership of region_tensor,...">GVA::VideoFrame::add_region</a>, you should not add other regions with underlying <code>gst_buffer_add_video_region_of_interest_meta</code> call. Results of future calls to methods of <a class="el" href="classGVA_1_1VideoFrame.html" title="This class represents video frame - object for working with RegionOfInterest and Tensor objects which...">GVA::VideoFrame</a> can be not expected due to broken <a class="el" href="classGVA_1_1VideoFrame.html" title="This class represents video frame - object for working with RegionOfInterest and Tensor objects which...">GVA::VideoFrame</a> invariant.</p>
<h2>Inference results flow</h2>
<p>Video analytics pipeline is a GStreamer pipeline with one or several GVA elements for inference and additional actions (publishing, rendering, etc.) if needed. Take a look at this pipeline: </p><div class="fragment"><div class="line">INPUT=video.mp4</div><div class="line">MODEL1=face-detection-adas-0001</div><div class="line">MODEL2=age-gender-recognition-retail-0013</div><div class="line">MODEL3=emotions-recognition-retail-0003</div><div class="line"></div><div class="line">gst-launch-1.0 \</div><div class="line">    filesrc location=${INPUT} ! decodebin ! video/x-raw ! videoconvert ! \</div><div class="line">    gvadetect   model=$(MODEL_PATH $MODEL1) ! queue ! \</div><div class="line">    gvaclassify model=$(MODEL_PATH $MODEL2) model-proc=$(PROC_PATH $MODEL2) ! queue ! \</div><div class="line">    gvaclassify model=$(MODEL_PATH $MODEL3) model-proc=$(PROC_PATH $MODEL3) ! queue ! \</div><div class="line">    gvawatermark ! videoconvert ! fpsdisplaysink video-sink=xvimagesink sync=false</div></div><!-- fragment --><p> Note: following explanation is based on <b>C++</b> GVA API, but <b>Python</b> GVA API can be used in the same way</p>
<p>Here, <b>gvadetect</b> runs face detection on each frame (<a class="el" href="classGVA_1_1VideoFrame.html" title="This class represents video frame - object for working with RegionOfInterest and Tensor objects which...">GVA::VideoFrame</a>) of provided video file. Let's say, it detected 3 faces on particular frame. It means, that 3 instances of <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a> will be added to this frame. Also, each <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a> will get its own detection <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a> attached. Such <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a> contains additional detection information.</p>
<p>After detection frame goes into first of two <b>gvaclassify</b> elements, which will iterate by 3 instances of <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a> added to <a class="el" href="classGVA_1_1VideoFrame.html" title="This class represents video frame - object for working with RegionOfInterest and Tensor objects which...">GVA::VideoFrame</a>. For each <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a> it will take region itself from full frame and run age &amp; gender classification on this region. Thus, each person detected will also get age and gender classified. It's achieved by adding 2 <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a> instances to current <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a> (one for age and one for gender).</p>
<p>After first classification frame goes into second <b>gvaclassify</b> element, which does the same job as previous <b>gvaclassify</b> element, but it classifies person's emotion, instead of age &amp; gender. Result emotion will be contained in its own <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a> added to current <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a>.</p>
<p>Thus, after detection &amp; classification part of pipeline we have <a class="el" href="classGVA_1_1VideoFrame.html" title="This class represents video frame - object for working with RegionOfInterest and Tensor objects which...">GVA::VideoFrame</a> with 3 instances of <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a> added, and 4 instances of <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a> added to each of 3 instances of <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a>. On image level, it means that we've got 3 persons detected, and age, gender &amp; emotion of each of them is classified.</p>
<p>Last GVA element in the pipeline is <b>gvawatermark</b> and its job is to display inference results on top of frame currently rendered on screen. To do this, it creates <a class="el" href="classGVA_1_1VideoFrame.html" title="This class represents video frame - object for working with RegionOfInterest and Tensor objects which...">GVA::VideoFrame</a> instance for current frame and iterates by attached <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a> instances, and by <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a> instances, attached to each of <a class="el" href="classGVA_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">GVA::RegionOfInterest</a>. This way, you will see bounding boxes drawed around persons faces, and these boxes will also be labeled with age, gender &amp; emotion.</p>
<p>The whole point of GVA API is to allow you to access any object we described above in any point of pipeline. You can read it, modify it, add your own tensors and regions of interest, and so on. Thus, you can add custom post-processing for any deep learning model in case GVA inference elements don't support it. There is a handy <b>gvainference</b> element, which adds raw inference result (output layer blob) in <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a>, so you can do custom post-processing on it.</p>
<h2>Print inference results example</h2>
<p>Here we provide a few code snippets, which print some of inference results obtained from video analytics pipeline. They are different ways to achieve the same thing. To get thorough understanding of how to create an application which takes advantage of GVA API, make sure to check out the rest of this document.</p>
<h3>1. Access inference results with low-level C code</h3>
<p>This is not recommended way of dealing with inference results obtained from video analytics pipeline and should be avoided unless some specific considerations exist </p><div class="fragment"><div class="line"><span class="preprocessor">#include &lt;gst/video/video.h&gt;</span> <span class="comment">// contains GstVideoRegionOfInterestMeta</span></div><div class="line"></div><div class="line"><span class="keywordtype">void</span> PrintMeta(GstBuffer *buffer) {</div><div class="line">    gpointer state = NULL;</div><div class="line">    GstMeta *meta = NULL;</div><div class="line">    <span class="keywordflow">while</span> ((meta = gst_buffer_iterate_meta(buffer, &amp;state)) != NULL) { <span class="comment">// iterate by meta attached to buffer</span></div><div class="line">        <span class="keywordflow">if</span> (meta-&gt;info-&gt;api != GST_VIDEO_REGION_OF_INTEREST_META_API_TYPE)</div><div class="line">            <span class="keywordflow">continue</span>; <span class="comment">// we only interested in GstVideoRegionOfInterestMeta instances, because it&#39;s produced by gvadetect</span></div><div class="line">        GstVideoRegionOfInterestMeta *roi_meta = (GstVideoRegionOfInterestMeta*)meta;</div><div class="line">        printf(<span class="stringliteral">&quot;Object bounding box %d,%d,%d,%d\n&quot;</span>, roi_meta-&gt;x, roi_meta-&gt;y, roi_meta-&gt;w, roi_meta-&gt;h);</div><div class="line">        <span class="keywordflow">for</span> (GList *l = roi_meta-&gt;params; l; l = g_list_next(l)) { <span class="comment">// iterate by tensors attached to this region of interest</span></div><div class="line">            GstStructure *structure = (GstStructure *) l-&gt;data;</div><div class="line">            if (gst_structure_has_name(structure, <span class="stringliteral">&quot;detection&quot;</span>))</div><div class="line">                <span class="keywordflow">continue</span>; <span class="comment">// detection tensor doesn&#39;t contain classification result and hence doesn&#39;t contain label</span></div><div class="line">            <span class="comment">// print some tensor information</span></div><div class="line">            printf(<span class="stringliteral">&quot;  Attribute %s\n&quot;</span>, gst_structure_get_name(structure));</div><div class="line">            <span class="keywordflow">if</span> (gst_structure_has_field(structure, <span class="stringliteral">&quot;label&quot;</span>)) {</div><div class="line">                printf(<span class="stringliteral">&quot;    label=%s\n&quot;</span>, gst_structure_get_string(structure, <span class="stringliteral">&quot;label&quot;</span>));</div><div class="line">            }</div><div class="line">            <span class="keywordflow">if</span> (gst_structure_has_field(structure, <span class="stringliteral">&quot;confidence&quot;</span>)) {</div><div class="line">                <span class="keywordtype">double</span> confidence;</div><div class="line">                gst_structure_get_double(structure, <span class="stringliteral">&quot;confidence&quot;</span>, &amp;confidence);</div><div class="line">                printf(<span class="stringliteral">&quot;    confidence=%.2f\n&quot;</span>, confidence);</div><div class="line">            }</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --> <h3>2. Access inference results with C++ API</h3>
<p>This is recommended way of dealing with inference results obtained from video analytics pipeline from C++ application </p><div class="fragment"><div class="line"><span class="preprocessor">#include &quot;<a class="code" href="video__frame_8h.html">video_frame.h</a>&quot;</span> <span class="comment">// contains GVA::VideoFrame, GVA::RegionOfInterest and GVA::Tensor</span></div><div class="line"></div><div class="line"><span class="keywordtype">void</span> PrintMeta(GstBuffer *buffer, GstCaps *caps) { <span class="comment">// simple function to display some tensor information</span></div><div class="line">    <a class="code" href="classGVA_1_1VideoFrame.html">GVA::VideoFrame</a> video_frame(buffer, caps);</div><div class="line">    std::vector&lt;GVA::RegionOfInterest&gt; regions = video_frame.regions();</div><div class="line">    <span class="keywordflow">for</span> (<a class="code" href="classGVA_1_1RegionOfInterest.html">GVA::RegionOfInterest</a> &amp;roi : regions) { <span class="comment">// iterate by regions of interest attached to this video frame</span></div><div class="line">        GstVideoRegionOfInterestMeta *meta = roi.meta(); <span class="comment">// get region of interest underlying meta to access bounding box information</span></div><div class="line">        std::cout &lt;&lt; <span class="stringliteral">&quot;Object bounding box &quot;</span> &lt;&lt; meta-&gt;x &lt;&lt; <span class="stringliteral">&quot;,&quot;</span> &lt;&lt; meta-&gt;y &lt;&lt; <span class="stringliteral">&quot;,&quot;</span> &lt;&lt; meta-&gt;w &lt;&lt; <span class="stringliteral">&quot;,&quot;</span> &lt;&lt; meta-&gt;h &lt;&lt; <span class="stringliteral">&quot;,&quot;</span> &lt;&lt; std::endl;</div><div class="line">        <span class="keywordflow">for</span> (<a class="code" href="classGVA_1_1Tensor.html">GVA::Tensor</a> &amp;tensor : roi) { <span class="comment">// iterate by tensors attached to this region of interest</span></div><div class="line">            <span class="keywordflow">if</span> tensor.is_detection()</div><div class="line">                <span class="keywordflow">continue</span>; <span class="comment">// detection tensor doesn&#39;t contain classification result and hence doesn&#39;t contain label</span></div><div class="line">            <span class="comment">// print some tensor information</span></div><div class="line">            std::cout &lt;&lt; <span class="stringliteral">&quot;  Attribute &quot;</span>     &lt;&lt; tensor.name()       &lt;&lt; std::endl;</div><div class="line">            std::cout &lt;&lt; <span class="stringliteral">&quot;    label= &quot;</span>      &lt;&lt; tensor.label()      &lt;&lt; std::endl;</div><div class="line">            std::cout &lt;&lt; <span class="stringliteral">&quot;    confidence= &quot;</span> &lt;&lt; tensor.confidence() &lt;&lt; std::endl;</div><div class="line">        }</div><div class="line">    }</div><div class="line">}</div></div><!-- fragment --><h3>3. Access inference results with Python API</h3>
<p>This is recommended way of dealing with inference results obtained from video analytics pipeline from Python code (via Python application or via <code>gvapython</code> element in pipeline) </p><div class="fragment"><div class="line">import gi</div><div class="line">gi.require_version(&#39;Gst&#39;, &#39;1.0&#39;)</div><div class="line">from gi.repository import Gst  # contains GStreamer entities</div><div class="line">from gstgva import VideoFrame</div><div class="line"></div><div class="line">def PrintMeta(buffer: Gst.Buffer, caps: Gst.Caps):  # simple function to display some tensor information</div><div class="line">    frame = VideoFrame(buffer, caps=caps)</div><div class="line">    for roi in frame.regions():  # iterate by regions of interest attached to this video frame</div><div class="line">        meta = roi.meta()  # get region of interest underlying meta to access bounding box information</div><div class="line">        print(&quot;Object bounding box {}, {}, {}, {}&quot;.format(meta.x, meta.y, meta.w, meta.h))</div><div class="line">        for tensor in roi:  # iterate by tensors attached to this region of interest</div><div class="line">            if tensor.is_detection():</div><div class="line">                continue  # detection tensor doesn&#39;t contain classification result and hence doesn&#39;t contain label</div><div class="line">            # print some tensor information</div><div class="line">            print(&quot;  Attribute {}&quot;.format(tensor.name()))</div><div class="line">            print(&quot;    label= {}&quot;.format(tensor.label()))</div><div class="line">            print(&quot;    confidence= {}&quot;.format(tensor.confidence()))</div></div><!-- fragment --><h2>Custom post-processing tutorial <a class="anchor" id="tutorial"></a></h2>
<p>There are several ways how you can access inference results provided by GVA elements in GStreamer pipeline using GVA API:</p><ol type="1">
<li>Create <b>C++/Python</b> application, which sets up callback on buffer passing through any GVA element and runs video analytics pipeline. In the body of this callback GstBuffer and, hence, GVA API will be available</li>
<li>Create <b>C++/Python</b> application, which runs video analytics pipeline with standard appsink element added as sink. You will then be able to register callback on GstBuffer incoming to appsink</li>
<li>Write your own GStreamer plugin which has access to GstBuffers coming through and insert it to pipeline after GVA inference elements</li>
</ol>
<p>We've got plenty examples of following ways 1 &amp; 2 in our samples and most existing GVA elements relying on existing inference result are basically based on 3rd way (e.g. <b>gvaclassify</b>, <b>gvatrack</b>, and more). Let's focus on one specific task: let's say, you have very specific deep learning model (not SSD and not YoloV3), which requires custom post-processing (<b>gvadetect</b> is not able to correctly interpret inference results of some models you can train or find on Web). You know how post processing should be implemented, but you don't know how to get and make any use of inference results produced by video analytics pipeline. To solve this task, you will need <b>gvainference</b> element, which runs deep learning model inference on passing video frame and stores raw inference result in a form of <a class="el" href="classGVA_1_1Tensor.html" title="This class represents tensor - map-like storage for inference result information, such as output blob...">GVA::Tensor</a>, attached to <a class="el" href="classGVA_1_1VideoFrame.html" title="This class represents video frame - object for working with RegionOfInterest and Tensor objects which...">GVA::VideoFrame</a>. So we use <b>gvainference</b> to get tensors, but how do we access these produced tensors?</p>
<p>Any of 3 approaches above will suffice. For the clarity of explanation, let's choose 1st one and focus on it. Also, for our tutorial we will add "custom" post-processing for SSD-like models. <b>gvadetect</b> already implements this type of post-processing, but here we will use <b>gvainference</b> and set up post-processing as callback. In your case, you will need to only put your post-processing code instead of ours.</p>
<p>Below you can see full snippet of <b>Python</b> code that is ready to solve your task. Take a look, and then we will talk about it closely. Note, that almost the same code can be written in <b>C++</b>. </p><div class="fragment"><div class="line">import sys</div><div class="line">from argparse import ArgumentParser</div><div class="line">import gi  # get Python bindings for GLib-based libraries</div><div class="line">gi.require_version(&#39;GstVideo&#39;, &#39;1.0&#39;)</div><div class="line">gi.require_version(&#39;Gst&#39;, &#39;1.0&#39;)</div><div class="line">gi.require_version(&#39;GObject&#39;, &#39;2.0&#39;)</div><div class="line">from gi.repository import Gst, GstVideo, GObject</div><div class="line"></div><div class="line"># GVA API modules</div><div class="line">from gstgva import VideoFrame, util</div><div class="line"></div><div class="line">parser = ArgumentParser(add_help=False)</div><div class="line">_args = parser.add_argument_group(&#39;Options&#39;)</div><div class="line">_args.add_argument(&quot;-i&quot;, &quot;--input&quot;, help=&quot;Required. Path to input video file&quot;,</div><div class="line">                   required=True, type=str)</div><div class="line">_args.add_argument(&quot;-d&quot;, &quot;--detection_model&quot;, help=&quot;Required. Path to an .xml file with object detection model&quot;,</div><div class="line">                   required=True, type=str)</div><div class="line"></div><div class="line"># init GStreamer</div><div class="line">Gst.init(sys.argv)</div><div class="line"></div><div class="line"># needed if you want to assign label string to your detections</div><div class="line">REGION_TENSOR = VideoFrame.create_labels_structure([&quot;background&quot;, &quot;face&quot;, &quot;my_label_2&quot;, &quot;etc.&quot;])</div><div class="line"></div><div class="line"># post-processing code</div><div class="line">def process_frame(frame: VideoFrame, threshold: float = 0.5) -&gt; bool:</div><div class="line">    width = frame.video_info().width</div><div class="line">    height = frame.video_info().height    </div><div class="line"></div><div class="line">    for tensor in frame.tensors():</div><div class="line">        dims = tensor.dims()</div><div class="line">        data = tensor.data()</div><div class="line">        object_size = dims[-1]</div><div class="line">        for i in range(dims[-2]):</div><div class="line">            image_id = data[i * object_size + 0]</div><div class="line">            confidence = data[i * object_size + 2]</div><div class="line">            x_min = int(data[i * object_size + 3] * width + 0.5)</div><div class="line">            y_min = int(data[i * object_size + 4] * height + 0.5)</div><div class="line">            x_max = int(data[i * object_size + 5] * width + 0.5)</div><div class="line">            y_max = int(data[i * object_size + 6] * height + 0.5)</div><div class="line"></div><div class="line">            if image_id != 0:</div><div class="line">                break</div><div class="line">            if confidence &lt; threshold:</div><div class="line">                continue</div><div class="line"></div><div class="line">            frame.add_region(x_min, y_min, x_max - x_min, y_max - y_min, 1, region_tensor=REGION_TENSOR)</div><div class="line"></div><div class="line">    return True</div><div class="line"></div><div class="line">def detect_postproc_callback(pad, info):</div><div class="line">    with util.GST_PAD_PROBE_INFO_BUFFER(info) as buffer:</div><div class="line">        caps = pad.get_current_caps()</div><div class="line">        frame = VideoFrame(buffer, caps=caps)</div><div class="line">        status = process_frame(frame)</div><div class="line">    return Gst.PadProbeReturn.OK if status else Gst.PadProbeReturn.DROP</div><div class="line"></div><div class="line">def main():</div><div class="line">    args = parser.parse_args()</div><div class="line"></div><div class="line">    # build pipeline using parse_launch</div><div class="line">    pipeline_str = &quot;filesrc location={} ! decodebin ! videoconvert ! video/x-raw,format=BGRx ! &quot; \</div><div class="line">        &quot;gvainference name=gvainference model={} ! queue ! &quot; \</div><div class="line">        &quot;gvawatermark ! videoconvert ! fpsdisplaysink video-sink=xvimagesink sync=false&quot;.format(</div><div class="line">            args.input, args.detection_model)</div><div class="line">    pipeline = Gst.parse_launch(pipeline_str)</div><div class="line"></div><div class="line">    # set callback</div><div class="line">    gvainference = pipeline.get_by_name(&quot;gvainference&quot;)</div><div class="line">    if gvainference:</div><div class="line">        pad = gvainference.get_static_pad(&quot;src&quot;)</div><div class="line">        pad.add_probe(Gst.PadProbeType.BUFFER, detect_postproc_callback)</div><div class="line"></div><div class="line">    # start pipeline</div><div class="line">    pipeline.set_state(Gst.State.PLAYING)</div><div class="line"></div><div class="line">    # wait until EOS or error</div><div class="line">    bus = pipeline.get_bus()</div><div class="line">    msg = bus.timed_pop_filtered(</div><div class="line">        Gst.CLOCK_TIME_NONE, Gst.MessageType.ERROR | Gst.MessageType.EOS)</div><div class="line"></div><div class="line">    # free pipeline</div><div class="line">    pipeline.set_state(Gst.State.NULL)</div><div class="line"></div><div class="line">if __name__ == &#39;__main__&#39;:</div><div class="line">    sys.exit(main() or 0)</div></div><!-- fragment --><p> Let's go through the most interesting pieces. First, we import necessary Python modules: </p><div class="fragment"><div class="line">import sys</div><div class="line">from argparse import ArgumentParser</div><div class="line">import gi  # get Python bindings for GLib-based libraries</div><div class="line">gi.require_version(&#39;GstVideo&#39;, &#39;1.0&#39;)</div><div class="line">gi.require_version(&#39;Gst&#39;, &#39;1.0&#39;)</div><div class="line">gi.require_version(&#39;GObject&#39;, &#39;2.0&#39;)</div><div class="line">from gi.repository import Gst, GstVideo, GObject</div><div class="line"></div><div class="line"># GVA API modules</div><div class="line">from gstgva import VideoFrame, util</div></div><!-- fragment --><p> Then, we parse command-line arguments. When run this script, you should specify input video with "-i" and your detection model with "-d": </p><div class="fragment"><div class="line">parser = ArgumentParser(add_help=False)</div><div class="line">_args = parser.add_argument_group(&#39;Options&#39;)</div><div class="line">_args.add_argument(&quot;-i&quot;, &quot;--input&quot;, help=&quot;Required. Path to input video file&quot;,</div><div class="line">                   required=True, type=str)</div><div class="line">_args.add_argument(&quot;-d&quot;, &quot;--detection_model&quot;, help=&quot;Required. Path to an .xml file with object detection model&quot;,</div><div class="line">                   required=True, type=str)</div></div><!-- fragment --><p> In the next piece we create REGION_TENSOR from list of labels. You probably know this list, because model was trained using this list. In our example, labels don't make much sense though. We will assume that our detection model classifies detection with label id set to 1, which matches "face" class (0 is for "background", 2 is for "my_label_2", 3 is for "etc.") </p><div class="fragment"><div class="line"># needed if you want to assign label string to your detections</div><div class="line">REGION_TENSOR = VideoFrame.create_labels_structure([&quot;background&quot;, &quot;face&quot;, &quot;my_label_2&quot;, &quot;etc.&quot;])</div></div><!-- fragment --><p> Next, function <code>process_frame</code> defines post-processing. As we said above, this code is for SSD-like models, so please feel freee to replace it with your own post-processing implementation that suffices your custom model. Meanwhile, let's take a look at usage of GVA API in this piece.</p>
<p>Tons of image information regarding current video frame can be obtain with <a class="el" href="classgstgva_1_1video__frame_1_1VideoFrame.html#ab20943e0804b2f72effefb587389877f" title="Get GstVideo.VideoInfo of this VideoFrame.">gstgva.video_frame.VideoFrame.video_info</a>. You can get image width, height, channels format and much more: </p><div class="fragment"><div class="line">width = frame.video_info().width</div><div class="line">height = frame.video_info().height    </div></div><!-- fragment --><p> Next, we iterate by <a class="el" href="classgstgva_1_1video__frame_1_1VideoFrame.html#ab465c4781d710e65ab449480effba6dc" title="Get Tensor objects attached to VideoFrame.">gstgva.video_frame.VideoFrame.tensors</a>, which were added by <b>gvainference</b>. We can get some inference result information, like <a class="el" href="classgstgva_1_1tensor_1_1Tensor.html#abc261e3b136831d4d039d20491997f15" title="Get inference result blob dimensions info.">gstgva.tensor.Tensor.dims</a> (list of model output blob dimensions) and <a class="el" href="classgstgva_1_1tensor_1_1Tensor.html#ada98d45397a28a562d10b1048f2e2e32" title="Get raw inference result blob data.">gstgva.tensor.Tensor.data</a> (raw output blob to interpret with your post-processing code): </p><div class="fragment"><div class="line">for tensor in frame.tensors():</div><div class="line">    dims = tensor.dims()</div><div class="line">    data = tensor.data()</div></div><!-- fragment --><p> After we eject bounding box parameters from raw inference blob, we are ready to <a class="el" href="classgstgva_1_1video__frame_1_1VideoFrame.html#a5b44069314287e3ee9dd42949395fbce" title="Attach RegionOfInterest to this VideoFrame.">gstgva.video_frame.VideoFrame.add_region</a> with box coordinates specified. Also, we specify <code>region_tensor</code> to <code>REGION_TENSOR</code> (which was created at the very start of this aplpication), and set <code>label_id</code> parameter to 1. This results to new <a class="el" href="classgstgva_1_1region__of__interest_1_1RegionOfInterest.html" title="This class represents region of interest - object describing detection result (bounding box) and cont...">gstgva.region_of_interest.RegionOfInterest</a> being added with label set to "face" </p><div class="fragment"><div class="line">frame.add_region(x_min, y_min, x_max - x_min, y_max - y_min, 1, region_tensor=REGION_TENSOR)</div></div><!-- fragment --><p> Next, we define callback which will run <code>process_frame</code> (our post-processing code) on each video frame passing by pipeline: </p><div class="fragment"><div class="line">def detect_postproc_callback(pad, info):</div><div class="line">    with util.GST_PAD_PROBE_INFO_BUFFER(info) as buffer:</div><div class="line">        caps = pad.get_current_caps()</div><div class="line">        frame = VideoFrame(buffer, caps=caps)</div><div class="line">        status = process_frame(frame)</div><div class="line">    return Gst.PadProbeReturn.OK if status else Gst.PadProbeReturn.DROP</div></div><!-- fragment --><p> In <code>main</code> function we create string template of video analytics pipeline with <b>gvainference</b> to run inference and <b>gvawatermark</b> to display bounding boxes and their labels (the ones we set to "face"): </p><div class="fragment"><div class="line"># build pipeline using parse_launch</div><div class="line">pipeline_str = &quot;filesrc location={} ! decodebin ! videoconvert ! video/x-raw,format=BGRx ! &quot; \</div><div class="line">    &quot;gvainference name=gvainference model={} ! queue ! &quot; \</div><div class="line">    &quot;gvawatermark ! videoconvert ! fpsdisplaysink video-sink=xvimagesink sync=false&quot;.format(</div><div class="line">    args.input, args.detection_model)</div></div><!-- fragment --><p> Finally, we register callback on <b>gvainference</b> source pad (source pad is meant to produce GstBuffer): </p><div class="fragment"><div class="line"># set callback</div><div class="line">gvainference = pipeline.get_by_name(&quot;gvainference&quot;)</div><div class="line">if gvainference:</div><div class="line">    pad = gvainference.get_static_pad(&quot;src&quot;)</div><div class="line">    pad.add_probe(Gst.PadProbeType.BUFFER, detect_postproc_callback)</div></div><!-- fragment --><p> Thus, before current frame leaves <b>gvainference</b>, <code>detect_postproc_callback</code> with access to GstBuffer will be called, where custom post-processing code is executed.</p>
<p>At the end of application, after all frames have passed by the pipeline, playback is finished. </p>
</div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.15
</small></address>
</body>
</html>
